#!/usr/bin/env python3
"""
OmniDocBench åˆ° LLaMAFactory çš„è½¬æ¢è„šæœ¬

åŠŸèƒ½ï¼š
1. è¯»å– OmniDocBench åŸå§‹æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼‰
2. åº”ç”¨æ€ç»´é“¾ç”Ÿæˆé€»è¾‘ï¼ˆåŸºäº layout æ ‡æ³¨ï¼‰
3. è½¬æ¢ä¸º LLaMAFactory ShareGPT æ ¼å¼
4. ç”Ÿæˆå¯¹åº”çš„ dataset_info.json

ä½¿ç”¨æ–¹æ³•ï¼š
    python omnidoc_to_llamafactory.py \
        --input data/omnidoc_raw.jsonl \
        --output data/omnidoc_processed.json \
        --dataset_name omnidoc_cot
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict, Any
try:
    from omnibench_enhanced import OmniDocConverter
except ImportError:
    from omnibench import OmniDocConverter


def convert_to_sharegpt(
    messages: List[Dict[str, Any]]
) -> Dict[str, Any]:
    """
    å°†è‡ªå®šä¹‰æ¶ˆæ¯æ ¼å¼è½¬æ¢ä¸º LLaMAFactory ShareGPT æ ¼å¼
    
    è¾“å…¥æ ¼å¼ï¼š
    {
        "messages": [
            {"role": "system", "content": [{"type": "text", "value": "..."}]},
            {"role": "user", "content": [{"type": "image", ...}, {"type": "text", ...}]},
            {"role": "assistant", "content": [{"type": "reasoning", ...}, {"type": "text", ...}]}
        ]
    }
    
    è¾“å‡ºæ ¼å¼ï¼š
    {
        "conversations": [
            {"from": "system", "value": "..."},
            {"from": "human", "value": "<image>\n..."},
            {"from": "gpt", "value": "<reasoning>\n...\n</reasoning>\n..."}
        ],
        "images": ["path/to/image.jpg"]
    }
    """
    conversations = []
    images = []
    
    role_map = {
        "system": "system",
        "user": "human",
        "assistant": "gpt"
    }
    
    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        
        from_role = role_map.get(role, role)
        text_parts = []
        
        for content_item in content:
            content_type = content_item["type"]
            
            if content_type == "image":
                # æå–å›¾åƒè·¯å¾„
                image_path = content_item.get("image_path", "")
                if image_path:
                    image_path = "images" + image_path
                    images.append(image_path)
                text_parts.append("<image>")
            
            elif content_type == "text":
                text_parts.append(content_item["value"])
            
            elif content_type == "reasoning":
                # æ€ç»´é“¾ç”¨ç‰¹æ®Šæ ‡è®°åŒ…è£¹
                reasoning_text = content_item["value"]
                text_parts.append(f"<reasoning>\n{reasoning_text}\n</reasoning>")
        
        conversations.append({
            "from": from_role,
            "value": "\n".join(text_parts)
        })
    
    result = {"conversations": conversations}
    if images:
        result["images"] = images
    
    return result


def process_omnidoc_file(
    input_file: str,
    output_file: str,
    max_samples: int = None
) -> int:
    """
    å¤„ç† OmniDocBench æ–‡ä»¶
    
    Args:
        input_file: è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„
        output_file: è¾“å‡º JSON æ–‡ä»¶è·¯å¾„
        max_samples: æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰
    
    Returns:
        å¤„ç†çš„æ ·æœ¬æ€»æ•°
    """
    print(f"ğŸ“– è¯»å–æ•°æ®: {input_file}")
    
    # è¯»å–åŸå§‹æ•°æ®
    raw_data = []
    with open(input_file, 'r', encoding='utf-8') as f:
        # æ£€æŸ¥æ–‡ä»¶æ ¼å¼
        first_char = f.read(1)
        f.seek(0)
        
        if first_char == '[':
            # JSONæ•°ç»„æ ¼å¼
            print("æ£€æµ‹åˆ°JSONæ•°ç»„æ ¼å¼ï¼Œæ­£åœ¨è§£æ...")
            data = json.load(f)
            raw_data = data[:max_samples] if max_samples else data
            if max_samples and len(data) > max_samples:
                print(f"âš ï¸  è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶: {max_samples}")
        else:
            # JSONLæ ¼å¼ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰
            print("æ£€æµ‹åˆ°JSONLæ ¼å¼ï¼Œæ­£åœ¨é€è¡Œè§£æ...")
            for idx, line in enumerate(f):
                if line.strip():
                    raw_data.append(json.loads(line))
                    if max_samples and len(raw_data) >= max_samples:
                        print(f"âš ï¸  è¾¾åˆ°æœ€å¤§æ ·æœ¬æ•°é™åˆ¶: {max_samples}")
                        break
    
    print(f"âœ“ è¯»å–äº† {len(raw_data)} é¡µæ•°æ®")
    
    # åº”ç”¨è½¬æ¢
    print("ğŸ”„ åº”ç”¨æ€ç»´é“¾è½¬æ¢...")
    converter = OmniDocConverter()
    all_samples = []
    
    for idx, page_data in enumerate(raw_data):
        try:
            samples = converter.process_page(page_data)
            all_samples.extend(samples)
            
            if (idx + 1) % 100 == 0:
                print(f"  å¤„ç†è¿›åº¦: {idx + 1}/{len(raw_data)} é¡µ")
        
        except Exception as e:
            print(f"âš ï¸  å¤„ç†ç¬¬ {idx} é¡µæ—¶å‡ºé”™: {e}")
            continue
    
    print(f"âœ“ ç”Ÿæˆäº† {len(all_samples)} ä¸ªé—®ç­”å¯¹")
    
    # è½¬æ¢ä¸º ShareGPT æ ¼å¼
    print("ğŸ”„ è½¬æ¢ä¸º ShareGPT æ ¼å¼...")
    sharegpt_data = []
    
    for sample in all_samples:
        try:
            sharegpt_sample = convert_to_sharegpt(sample["messages"])
            sharegpt_data.append(sharegpt_sample)
        except Exception as e:
            print(f"âš ï¸  è½¬æ¢æ ·æœ¬æ—¶å‡ºé”™: {e}")
            continue
    
    # ä¿å­˜
    output_path = Path(output_file)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(sharegpt_data, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… ä¿å­˜æˆåŠŸ: {output_file}")
    print(f"   æ€»è®¡: {len(sharegpt_data)} æ¡è®­ç»ƒæ•°æ®")
    
    return len(sharegpt_data)


def create_dataset_info(
    dataset_name: str,
    file_name: str,
    output_dir: str
) -> None:
    """
    åˆ›å»º dataset_info.json æ–‡ä»¶
    
    Args:
        dataset_name: æ•°æ®é›†åç§°
        file_name: æ•°æ®æ–‡ä»¶åï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
        output_dir: è¾“å‡ºç›®å½•
    """
    dataset_info = {
        dataset_name: {
            "file_name": file_name,
            "formatting": "sharegpt",
            "columns": {
                "messages": "conversations",
                "images": "images"
            },
            "tags": {
                "role_tag": "from",
                "content_tag": "value",
                "user_tag": "human",
                "assistant_tag": "gpt",
                "system_tag": "system"
            }
        }
    }
    
    info_path = Path(output_dir) / "dataset_info.json"
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œåˆå¹¶
    if info_path.exists():
        with open(info_path, 'r', encoding='utf-8') as f:
            existing_info = json.load(f)
        existing_info.update(dataset_info)
        dataset_info = existing_info
    
    with open(info_path, 'w', encoding='utf-8') as f:
        json.dump(dataset_info, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… dataset_info.json å·²æ›´æ–°: {info_path}")


def preview_sample(output_file: str, num_samples: int = 2) -> None:
    """é¢„è§ˆç”Ÿæˆçš„æ ·æœ¬"""
    with open(output_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"\n{'='*60}")
    print(f"ğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰ {num_samples} æ¡ï¼‰")
    print(f"{'='*60}\n")
    
    for idx, sample in enumerate(data[:num_samples]):
        print(f"--- æ ·æœ¬ {idx + 1} ---")
        print(json.dumps(sample, indent=2, ensure_ascii=False))
        print()


def main():
    parser = argparse.ArgumentParser(
        description="å°† OmniDocBench æ•°æ®è½¬æ¢ä¸º LLaMAFactory æ ¼å¼"
    )
    
    parser.add_argument(
        "--input",
        type=str,
        required=True,
        help="è¾“å…¥ JSONL æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--output",
        type=str,
        required=True,
        help="è¾“å‡º JSON æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--dataset_name",
        type=str,
        default="omnidoc_cot",
        help="æ•°æ®é›†åç§°ï¼ˆç”¨äº dataset_info.jsonï¼‰"
    )
    
    parser.add_argument(
        "--max_samples",
        type=int,
        default=None,
        help="æœ€å¤§å¤„ç†æ ·æœ¬æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼‰"
    )
    
    parser.add_argument(
        "--preview",
        action="store_true",
        help="é¢„è§ˆç”Ÿæˆçš„æ ·æœ¬"
    )
    
    parser.add_argument(
        "--no_dataset_info",
        action="store_true",
        help="ä¸ç”Ÿæˆ dataset_info.json"
    )
    
    args = parser.parse_args()
    
    print("="*60)
    print("OmniDocBench â†’ LLaMAFactory è½¬æ¢å·¥å…·")
    print("="*60)
    print()
    
    # å¤„ç†æ•°æ®
    num_samples = process_omnidoc_file(
        args.input,
        args.output,
        args.max_samples
    )
    
    # ç”Ÿæˆ dataset_info.json
    if not args.no_dataset_info:
        output_dir = Path(args.output).parent
        file_name = Path(args.output).name
        create_dataset_info(args.dataset_name, file_name, output_dir)
    
    # é¢„è§ˆ
    if args.preview and num_samples > 0:
        preview_sample(args.output)
    
    print("\n" + "="*60)
    print("âœ… è½¬æ¢å®Œæˆï¼")
    print("="*60)
    print(f"\nä¸‹ä¸€æ­¥ï¼šä½¿ç”¨ LLaMAFactory è®­ç»ƒ")
    print(f"  llamafactory-cli train \\")
    print(f"    --model_name_or_path Qwen/Qwen-VL-Chat \\")
    print(f"    --dataset {args.dataset_name} \\")
    print(f"    --dataset_dir {Path(args.output).parent} \\")
    print(f"    --output_dir ./output")
    print()


if __name__ == "__main__":
    main()
