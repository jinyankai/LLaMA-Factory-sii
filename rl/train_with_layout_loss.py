#!/usr/bin/env python3
"""
ä½¿ç”¨å¸ƒå±€æ„ŸçŸ¥æŸå¤±è®­ç»ƒ Qwen-VL æ¨¡å‹

è¿™ä¸ªè„šæœ¬å±•ç¤ºäº†å¦‚ä½•ï¼š
1. åŠ è½½ OmniDocBench å¤„ç†åçš„æ•°æ®
2. ä½¿ç”¨è‡ªå®šä¹‰çš„å¸ƒå±€æ„ŸçŸ¥æŸå¤±å‡½æ•°
3. è®­ç»ƒå¤šæ¨¡æ€æ–‡æ¡£ç†è§£æ¨¡å‹

ä½¿ç”¨æ–¹æ³•ï¼š
    python train_with_layout_loss.py \
        --model_name Qwen/Qwen-VL-Chat \
        --data_file data/omnidoc_processed.json \
        --output_dir ./output \
        --num_epochs 3
"""

import os
import json
import argparse
from typing import Dict, List, Any
from pathlib import Path

import torch
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
)
from datasets import Dataset, load_dataset

# å¯¼å…¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°
from layout_aware_loss import LayoutAwareLoss


# ==========================================
# 1. æ•°æ®å¤„ç†
# ==========================================

def load_and_prepare_dataset(data_file: str, tokenizer, max_length: int = 2048):
    """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®é›†"""
    
    print(f"ğŸ“– åŠ è½½æ•°æ®: {data_file}")
    
    # åŠ è½½ JSON æ•°æ®
    with open(data_file, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"âœ“ åŠ è½½äº† {len(data)} æ¡æ•°æ®")
    
    # è½¬æ¢ä¸º HuggingFace Dataset
    dataset = Dataset.from_list(data)
    
    # é¢„å¤„ç†å‡½æ•°
    def preprocess_function(examples):
        """å°† ShareGPT æ ¼å¼è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥"""
        
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ¨¡å‹æ ¼å¼è°ƒæ•´
        # å¯¹äº Qwen-VLï¼Œé€šå¸¸éœ€è¦ç‰¹æ®Šçš„æ ¼å¼åŒ–
        
        # ç®€åŒ–ç¤ºä¾‹ï¼šæ‹¼æ¥å¯¹è¯
        texts = []
        for conversations in examples["conversations"]:
            text = ""
            for turn in conversations:
                role = turn["from"]
                content = turn["value"]
                
                if role == "system":
                    text += f"System: {content}\n"
                elif role == "human":
                    text += f"Human: {content}\n"
                elif role == "gpt":
                    text += f"Assistant: {content}\n"
            
            texts.append(text)
        
        # Tokenize
        model_inputs = tokenizer(
            texts,
            max_length=max_length,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        
        # åˆ›å»º labelsï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰
        model_inputs["labels"] = model_inputs["input_ids"].clone()
        
        return model_inputs
    
    # åº”ç”¨é¢„å¤„ç†
    print("ğŸ”„ é¢„å¤„ç†æ•°æ®...")
    processed_dataset = dataset.map(
        preprocess_function,
        batched=True,
        remove_columns=dataset.column_names,
        desc="Tokenizing"
    )
    
    print(f"âœ“ é¢„å¤„ç†å®Œæˆ")
    
    return processed_dataset


# ==========================================
# 2. è‡ªå®šä¹‰ Trainer
# ==========================================

class LayoutAwareTrainer(Trainer):
    """å¸¦æœ‰å¸ƒå±€æ„ŸçŸ¥æŸå¤±çš„ Trainer"""
    
    def __init__(self, *args, loss_config: Dict[str, Any] = None, **kwargs):
        super().__init__(*args, **kwargs)
        
        # åˆå§‹åŒ–è‡ªå®šä¹‰æŸå¤±å‡½æ•°
        self.layout_loss_fn = LayoutAwareLoss(**(loss_config or {}))
        
        # æ§åˆ¶æ˜¯å¦è§£ç æ–‡æœ¬ï¼ˆè®­ç»ƒæ—¶å…³é—­ä»¥æé€Ÿï¼‰
        self.decode_for_layout_loss = False
        
        print("âœ“ ä½¿ç”¨å¸ƒå±€æ„ŸçŸ¥æŸå¤±å‡½æ•°")
        print(f"  - è¯­è¨€å»ºæ¨¡æƒé‡: {self.layout_loss_fn.alpha}")
        print(f"  - è¾¹ç•Œæ¡†æƒé‡: {self.layout_loss_fn.beta}")
        print(f"  - å…³ç³»æƒé‡: {self.layout_loss_fn.gamma}")
        print(f"  - é¡ºåºæƒé‡: {self.layout_loss_fn.delta}")
    
    def compute_loss(self, model, inputs, return_outputs=False):
        """è‡ªå®šä¹‰æŸå¤±è®¡ç®—"""
        
        # å‰å‘ä¼ æ’­
        outputs = model(**inputs)
        logits = outputs.logits
        labels = inputs.get("labels")
        
        # è§£ç æ–‡æœ¬ï¼ˆå¯é€‰ï¼Œç”¨äºå¸ƒå±€æŸå¤±ï¼‰
        pred_texts = None
        target_texts = None
        
        if self.decode_for_layout_loss:
            # è·å–é¢„æµ‹
            pred_ids = torch.argmax(logits, dim=-1)
            
            # è§£ç 
            pred_texts = self.tokenizer.batch_decode(
                pred_ids,
                skip_special_tokens=True
            )
            
            # è§£ç ç›®æ ‡ï¼ˆå¿½ç•¥ -100ï¼‰
            target_ids = labels.clone()
            target_ids[target_ids == -100] = self.tokenizer.pad_token_id
            target_texts = self.tokenizer.batch_decode(
                target_ids,
                skip_special_tokens=True
            )
        
        # è®¡ç®—æŸå¤±
        loss_dict = self.layout_loss_fn(
            logits=logits,
            labels=labels,
            pred_texts=pred_texts,
            target_texts=target_texts
        )
        
        # è®°å½•å„é¡¹æŸå¤±
        self.log({
            "lm_loss": loss_dict["lm_loss"].item(),
            "bbox_loss": loss_dict["bbox_loss"].item(),
            "relation_loss": loss_dict["relation_loss"].item(),
            "order_loss": loss_dict["order_loss"].item(),
        })
        
        loss = loss_dict["loss"]
        
        return (loss, outputs) if return_outputs else loss


# ==========================================
# 3. ä¸»è®­ç»ƒå‡½æ•°
# ==========================================

def main():
    parser = argparse.ArgumentParser(description="ä½¿ç”¨å¸ƒå±€æ„ŸçŸ¥æŸå¤±è®­ç»ƒæ¨¡å‹")
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument("--model_name", type=str, default="Qwen/Qwen-VL-Chat",
                        help="æ¨¡å‹åç§°æˆ–è·¯å¾„")
    parser.add_argument("--data_file", type=str, required=True,
                        help="è®­ç»ƒæ•°æ®æ–‡ä»¶ï¼ˆJSON æ ¼å¼ï¼‰")
    parser.add_argument("--output_dir", type=str, default="./output",
                        help="è¾“å‡ºç›®å½•")
    
    # è®­ç»ƒå‚æ•°
    parser.add_argument("--num_epochs", type=int, default=3,
                        help="è®­ç»ƒè½®æ•°")
    parser.add_argument("--batch_size", type=int, default=2,
                        help="æ¯è®¾å¤‡æ‰¹æ¬¡å¤§å°")
    parser.add_argument("--gradient_accumulation_steps", type=int, default=8,
                        help="æ¢¯åº¦ç´¯ç§¯æ­¥æ•°")
    parser.add_argument("--learning_rate", type=float, default=5e-5,
                        help="å­¦ä¹ ç‡")
    parser.add_argument("--max_length", type=int, default=2048,
                        help="æœ€å¤§åºåˆ—é•¿åº¦")
    
    # æŸå¤±å‡½æ•°å‚æ•°
    parser.add_argument("--alpha", type=float, default=1.0,
                        help="è¯­è¨€å»ºæ¨¡æŸå¤±æƒé‡")
    parser.add_argument("--beta", type=float, default=0.5,
                        help="è¾¹ç•Œæ¡†æŸå¤±æƒé‡")
    parser.add_argument("--gamma", type=float, default=0.3,
                        help="å…³ç³»æŸå¤±æƒé‡")
    parser.add_argument("--delta", type=float, default=0.2,
                        help="é¡ºåºæŸå¤±æƒé‡")
    parser.add_argument("--bbox_loss_type", type=str, default="smooth_l1",
                        choices=["smooth_l1", "iou"],
                        help="è¾¹ç•Œæ¡†æŸå¤±ç±»å‹")
    
    # å…¶ä»–å‚æ•°
    parser.add_argument("--fp16", action="store_true",
                        help="ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒ")
    parser.add_argument("--eval_split", type=float, default=0.1,
                        help="éªŒè¯é›†æ¯”ä¾‹")
    parser.add_argument("--seed", type=int, default=42,
                        help="éšæœºç§å­")
    
    args = parser.parse_args()
    
    print("="*60)
    print("å¸ƒå±€æ„ŸçŸ¥æŸå¤±è®­ç»ƒè„šæœ¬")
    print("="*60)
    print(f"æ¨¡å‹: {args.model_name}")
    print(f"æ•°æ®: {args.data_file}")
    print(f"è¾“å‡º: {args.output_dir}")
    print("="*60)
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    
    # 1. åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
    print("\nğŸ“¦ åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
    
    tokenizer = AutoTokenizer.from_pretrained(
        args.model_name,
        trust_remote_code=True
    )
    
    model = AutoModelForCausalLM.from_pretrained(
        args.model_name,
        torch_dtype=torch.float16 if args.fp16 else torch.float32,
        device_map="auto",
        trust_remote_code=True
    )
    
    print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ")
    print(f"  å‚æ•°é‡: {sum(p.numel() for p in model.parameters()) / 1e9:.2f}B")
    
    # 2. åŠ è½½æ•°æ®
    dataset = load_and_prepare_dataset(
        args.data_file,
        tokenizer,
        max_length=args.max_length
    )
    
    # åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†
    if args.eval_split > 0:
        split_dataset = dataset.train_test_split(
            test_size=args.eval_split,
            seed=args.seed
        )
        train_dataset = split_dataset["train"]
        eval_dataset = split_dataset["test"]
        print(f"âœ“ æ•°æ®åˆ’åˆ†: è®­ç»ƒ {len(train_dataset)} / éªŒè¯ {len(eval_dataset)}")
    else:
        train_dataset = dataset
        eval_dataset = None
        print(f"âœ“ è®­ç»ƒæ•°æ®: {len(train_dataset)}")
    
    # 3. è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.num_epochs,
        per_device_train_batch_size=args.batch_size,
        per_device_eval_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        fp16=args.fp16,
        logging_dir=f"{args.output_dir}/logs",
        logging_steps=10,
        save_steps=100,
        save_total_limit=3,
        evaluation_strategy="steps" if eval_dataset else "no",
        eval_steps=100 if eval_dataset else None,
        load_best_model_at_end=True if eval_dataset else False,
        metric_for_best_model="loss" if eval_dataset else None,
        report_to="tensorboard",
        seed=args.seed,
    )
    
    # 4. æŸå¤±å‡½æ•°é…ç½®
    loss_config = {
        "alpha": args.alpha,
        "beta": args.beta,
        "gamma": args.gamma,
        "delta": args.delta,
        "bbox_loss_type": args.bbox_loss_type,
        "normalize_coords": True,
        "page_size": (1200, 1684),  # æ ¹æ®ä½ çš„æ•°æ®è°ƒæ•´
    }
    
    # 5. åˆ›å»º Trainer
    print("\nğŸš€ åˆå§‹åŒ– Trainer...")
    
    trainer = LayoutAwareTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=eval_dataset,
        tokenizer=tokenizer,
        loss_config=loss_config,
    )
    
    # 6. è®­ç»ƒ
    print("\nğŸ¯ å¼€å§‹è®­ç»ƒ...")
    print(f"  æ€»æ­¥æ•°: {len(train_dataset) // (args.batch_size * args.gradient_accumulation_steps) * args.num_epochs}")
    print(f"  æœ‰æ•ˆæ‰¹æ¬¡å¤§å°: {args.batch_size * args.gradient_accumulation_steps}")
    print()
    
    train_result = trainer.train()
    
    # 7. ä¿å­˜æ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    trainer.save_model(args.output_dir)
    tokenizer.save_pretrained(args.output_dir)
    
    # ä¿å­˜è®­ç»ƒæŒ‡æ ‡
    metrics = train_result.metrics
    trainer.log_metrics("train", metrics)
    trainer.save_metrics("train", metrics)
    
    print("\n" + "="*60)
    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"æ¨¡å‹ä¿å­˜åœ¨: {args.output_dir}")
    print(f"æ—¥å¿—ä¿å­˜åœ¨: {args.output_dir}/logs")
    print("\næŸ¥çœ‹è®­ç»ƒæ—¥å¿—:")
    print(f"  tensorboard --logdir {args.output_dir}/logs")
    print()


if __name__ == "__main__":
    main()
